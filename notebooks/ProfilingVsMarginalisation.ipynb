{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A simple counting experiment\n",
    "\n",
    "For our simple \"HEP\"-inspired counting experiment from the lectures, we can write the likelihood as, \n",
    "\n",
    "$$\n",
    "L(\\mu,\\eta) = \\lambda(\\mu,\\eta)^{n}e^{-\\lambda}\\cdot e^{-\\frac{1}{2}\\eta^{2}}\n",
    "$$\n",
    "\n",
    "where we've dropped the $n!$ in the likelihood (for reasons that will be clear in the future). $\\mu$ is the parameter of interest (relating our measured cross-section to some hypothetical one) and $\\eta$ is our nuisance parameter that encodes our uncertainty on the luminosity, \n",
    "\n",
    "$$\n",
    "\\lambda(\\mu,\\eta) = \\mu\\sigma(pp\\rightarrow \\mathrm{X})A\\epsilon l_{0}\\cdot(1+\\kappa)^{\\eta}+B\n",
    "$$\n",
    "\n",
    "We'll assume that we have some specified values given to us. And these have been defined in a file called `counting_model.py`. We can import that to access everything in  the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "f=open(\"counting_model.py\")\n",
    "for line in f.readlines(): print(line),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And with these we can specify the Poisson term,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from counting_model import *\n",
    "\n",
    "# Poisson mean\n",
    "def lamb(mu,eta):\n",
    "  return mu*eff*A*l*((1+k)**eta)*sigma_TH + B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our aim here will be to remove the dependence of the likelihood (and therefore our infrence on $\\mu$) on the parameter $\\eta$. As explained in lectures, there are two ways  to procede common in HEP, profiling, or marginaisation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Profiling\n",
    "\n",
    "Profiling is the method of  removing the dependence on $\\eta$ through finding the values of $\\eta$ for which $L(\\mu,\\eta)$ is maximised at each value of $\\mu$. Thus $\\eta$ becomes a function of $\\mu$, i.e \n",
    "\n",
    "$$\n",
    "L(\\mu,\\eta)\\rightarrow L(\\mu,\\eta(\\mu))\n",
    "$$\n",
    "\n",
    "Often, since we deal with exponentials, its easier to minimize a negative log-likelihood, than maximise a likelihood. So let's write down the negative log-likelihood function. In fact by convention we also multiply by two (more on that in lectures later). \n",
    "\n",
    "$$\n",
    "q(\\mu,\\eta) = -2\\ln L(\\mu,\\eta)  = \\eta^{2}+2\\lambda(\\mu,eta)-2n\\ln\\lambda(\\mu,\\eta)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q = -2lnL\n",
    "def q(mu,eta):\n",
    "  la = lamb(mu,eta)\n",
    "  return eta*eta + 2*la -2*n*numpy.log(la)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To profile the function, we need to find value(s) of $\\eta$ for which,\n",
    "\n",
    "$$\n",
    "q{\\prime} = \\frac{\\partial q}{\\partial \\eta} = 0  \n",
    "$$\n",
    "\n",
    "We'll use a numerial method known as the \"Newton method\" to solve this. This method works as follows; \n",
    "\n",
    "   * Choose an initial starting point for the parameter, call it $\\eta_{0}$.\n",
    "   * The next point proposed is $\\eta_{1}=\\eta_{0}-\\frac{q\\prime(\\eta_{0})}{q\\prime\\prime(\\eta_{0})}$. This new point now replaces $\\eta_{0}$/ \n",
    "   * Continue iterating until $|q\\prime|<\\delta$, where $\\delta$  is some pre-defined tolerance\n",
    "   \n",
    "Since we can analytically calculate them, we also write the functions for $q\\prime$  and $q\\prime\\prime$,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dq/deta\n",
    "def dq(mu,eta):\n",
    "  la = lamb(mu,eta)\n",
    "  dl = (la-B)*numpy.log(1+k)\n",
    "  return 2.*eta + 2.*dl - 2.*n*dl/la\n",
    "\n",
    "# d^2q/deta^2\n",
    "def d2q(mu,eta):\n",
    "  log_k = numpy.log(1+k)\n",
    "  la    = lamb(mu,eta)\n",
    "  dl    = log_k*(la-B)\n",
    "  d2l   = log_k*log_k*(la-B)\n",
    "    \n",
    "  return 2. + 2.*d2l - 2.*n*(1./(la*la))*dl*dl - 2.*n*(1./la)*d2l\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, lets put this into a function which returns the profiled value of $\\eta$ for a given value of $\\mu$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def profiled_eta(mu):\n",
    "  # numerical minimumisation via newton method\n",
    "  tol = 0.01\n",
    "  #init_eta =-0.5 if lamb(mu,0)-n > 0 else 0.5\n",
    "  init_eta = 0.\n",
    "  eps = 100\n",
    "  while eps > tol:\n",
    "    qp = dq(mu,init_eta)\n",
    "    eps = abs(qp)\n",
    "    init_eta = init_eta - qp/d2q(mu,init_eta)\n",
    "  return init_eta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how it looks  now. First, we can plot the  value of $q(\\mu,\\eta)$ in a 2D color map.\n",
    "\n",
    "We then call our `profiled_eta` function to plot the value of eta which minimizes $q(\\mu,\\eta)$ for each value of $\\mu$ in a reasonably wide range. \n",
    "\n",
    "Finally, by plugging these values of $\\eta$ into our function for $q$, we obtain the *profiled negative log-likelihood* as a function of $\\mu$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting\n",
    "fig, (ax1,ax2) = plt.subplots(1,2,figsize=(14,6))\n",
    "\n",
    "# 1. plot q(mu,eta)\n",
    "xaxis = numpy.linspace(0.1,15,50)\n",
    "yaxis = numpy.linspace(-3,3,50)\n",
    "z = [ [q(mu,eta) for mu in xaxis] for eta in yaxis ]\n",
    "X,Y = numpy.meshgrid(xaxis,yaxis)\n",
    "c = ax1.pcolor(X,Y,z)\n",
    "fig.colorbar(c,ax=ax1)\n",
    "ax1.set_xlabel(\"$\\\\mu$\")\n",
    "ax1.set_ylabel(\"$\\eta$\")\n",
    "ax1.set_title(\"$q(\\\\mu,\\eta)$\")\n",
    "\n",
    "# 2. plot the profiled value of eta as a function of mu\n",
    "eta_mu = [ profiled_eta(mu) for mu in xaxis ]\n",
    "ax1.plot(xaxis,eta_mu,color='red')\n",
    "\n",
    "# 2. plot the profile likelihood\n",
    "q_mu = [ q(mu,eta) for mu,eta in zip(xaxis,eta_mu)]\n",
    "ax2.plot(xaxis,q_mu)\n",
    "ax2.set_xlabel(\"$\\\\mu$\")\n",
    "ax2.set_ylabel(\"$q(\\\\mu)$\")\n",
    "\n",
    "plt.show()\n",
    "plt.savefig(\"prof_lh_ex.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Marginalisation\n",
    "\n",
    "In Bayesian statistics, the way to eliminate nuisance parameters is through *integrating* over the joint posterior distribution with respect to the nuisance parameters. This is known as *marginalisation*.  \n",
    "\n",
    "For a single parameter of interest ($\\mu$) and a single nuisance parameter ($\\eta$), we have for our counting experiment, \n",
    "\n",
    "$$\n",
    "p(\\mu,\\eta|n) = \\frac{p(n|\\mu,\\eta)p(\\mu,\\eta)}{p(n)} =  \\frac{p(n|\\mu,\\eta)p(\\mu)p(\\eta)}{p(n)}\n",
    "$$\n",
    "\n",
    "and therefore,  \n",
    "\n",
    "$$\n",
    "p(\\mu,\\eta|n) = \\int \\frac{p(n|\\mu,\\eta)p(\\mu)p(\\eta)}{p(n)}  d\\eta\n",
    "$$\n",
    "\n",
    "where we have assimed that for our counting experiment, the prior on $\\eta$ is is independent of $\\mu$. We will assume a  *flat* prior for the parameter of interest too, and the prior on $\\eta$ is a Normal distribution so that;\n",
    "\n",
    "$$\n",
    "p(\\mu,\\eta) = p(\\mu)\\cdot p(\\eta) = \\frac{1}{20}\\cdot \\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{1}{2}\\eta^{2}}, \n",
    "$$\n",
    "if $\\mu\\in (0,20)$  and 0 otherwise. \n",
    "\n",
    "A usual, $p(n)=\\int p(\\mu|n)d\\mu = \\int\\int p(\\mu,\\eta|n)d\\mu d\\eta$.\n",
    "\n",
    "We introduce back the factorial term in the likelihood so that \n",
    "\n",
    "$$\n",
    "p(n|\\mu,\\eta)  = \\frac{L(\\mu,\\eta)}{n!}\n",
    "$$\n",
    "\n",
    "We can define the different terms as functions,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Likelihood --> P(n|mu,eta)\n",
    "def Likelihood(n,mu,eta):\n",
    "  l = lamb(mu,eta)\n",
    "  return (l**n)*numpy.exp(-l)/numpy.math.factorial(int(n))\n",
    "\n",
    "# prior on eta P(eta)\n",
    "def prior_eta(eta): \n",
    "  c = 1./((2*numpy.pi)**0.5)\n",
    "  return c*numpy.exp(-0.5*eta*eta)\n",
    "\n",
    "# prior on mu P(mu)\n",
    "def prior_mu(mu):\n",
    "  return 1./20 if (mu < 20 and mu > 0) else 0 \n",
    "\n",
    "# and define the product of them (the numerator in Bayes rule)\n",
    "def product(eta,mu,n):\n",
    "  return Likelihood(n,mu,eta)*prior_mu(mu)*prior_eta(eta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the `integrate.quad` method from `scipy` to do the intergration for us numerically, one for the integral over $\\eta$ and one for the integral over $\\eta$ and $\\mu$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.integrate as integrate\n",
    "\n",
    "# integrate over eta\n",
    "def integral(mu,n):\n",
    "  return integrate.quad(product,-6,6,args=(mu,n),epsabs=1.49e-03)[0]\n",
    "    \n",
    "# and then integrate over mu\n",
    "def norm(n):\n",
    "  return integrate.dblquad(product,0.01,22,lambda x:-6,lambda x:6,args=[n],epsabs=1.49e-03)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see what this looks like for our simple counting experiment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1,ax2) = plt.subplots(1,2,figsize=(14,6))\n",
    "\n",
    "# plot the product of the likelihood and priors\n",
    "xaxis = numpy.linspace(0.,22,200)\n",
    "yaxis = numpy.linspace(-3,3,200)\n",
    "z = [ [product(eta,mu,n) for mu in xaxis] for eta in yaxis ]\n",
    "X,Y = numpy.meshgrid(xaxis,yaxis) \n",
    "c = ax1.pcolor(X,Y,z)\n",
    "fig.colorbar(c,ax=ax1)\n",
    "ax1.set_xlabel(\"$\\\\mu$\")\n",
    "ax1.set_ylabel(\"$\\eta$\")\n",
    "ax1.set_title(\"$p(n|\\\\mu,\\eta)p(\\\\mu)p(\\eta)$\")\n",
    "\n",
    "normalise = norm(n)\n",
    "# plot the posterior\n",
    "p_mu = [ integral(mu,n)/normalise for mu in xaxis ]\n",
    "ax2.plot(xaxis,p_mu)\n",
    "ax2.set_xlabel(\"$\\\\mu$\")\n",
    "ax2.set_ylabel(\"$p(\\\\mu$)\")\n",
    "\n",
    "plt.show()\n",
    "plt.savefig(\"marginalised_lh_ex.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that we can also use the MarkovChainMC approach to do the intergration..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proposal(mu,eta):\n",
    "    return numpy.random.normal(mu,1),numpy.random.normal(eta,1)\n",
    "\n",
    "# function to accept new proposal or reject it \n",
    "def accept_or_reject(mu_new,eta_new,mu,eta):  \n",
    "    # check for 0 \n",
    "    if product(eta,mu,n) > 0: \n",
    "      r = product(eta_new,mu_new,n)/product(eta,mu,n)\n",
    "    else:\n",
    "      r = 2.\n",
    "    if r > 1: \n",
    "        return mu_new,eta_new,True \n",
    "    else:\n",
    "        alpha = min([r,1])\n",
    "        rnd = numpy.random.uniform(0,1)\n",
    "        if rnd < alpha : \n",
    "            return mu_new,eta_new,True\n",
    "        else: return mu,eta,False\n",
    "\n",
    "init = proposal(1,0)\n",
    "accepted_mc = []\n",
    "\n",
    "number_steps = 100000\n",
    "for i in range(number_steps) :\n",
    "    \n",
    "    mu_new,eta_new = proposal(init[0],init[1])\n",
    "    mu_new,eta_new,accept = accept_or_reject(mu_new,eta_new,init[0],init[1])\n",
    "    init = [mu_new,eta_new]\n",
    "    if accept: accepted_mc.append(mu_new)\n",
    "\n",
    "# plot the accepted points, overlay the numerical result - use a burn-in of 20,000 points\n",
    "accepted_mc_MH = accepted_mc[20000:-1]\n",
    "\n",
    "fig, (ax1,ax2) = plt.subplots(1,2,figsize=(14,6))\n",
    "ax1.plot(range(len(accepted_mc)),accepted_mc)\n",
    "ax2.hist(accepted_mc_MH,density=True,fill=False,edgecolor='blue',bins=50)\n",
    "\n",
    "ax2.plot(xaxis, p_mu)\n",
    "ax2.set_xlabel(\"$\\mu$\")\n",
    "ax2.set_ylabel(\"$p(\\mu|n)$\")\n",
    "plt.show()\n",
    "plt.savefig(\"counting_experiment_bayes_MCMC.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be using this simple counting experiment again so i'll save a bunch of the functions we used in `counting_model_functions.py`. We'll see later that we will need to throw pseudo-data - for us thats the value of $n$ and $\\eta^{\\prime}$ which here are 2 and 0 respectively. This means our functions need to take these as arguments but otherwise, are essentially the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
