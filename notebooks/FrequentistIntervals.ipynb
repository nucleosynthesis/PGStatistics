{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d7852e1",
   "metadata": {},
   "source": [
    "# Frequentist intervals\n",
    "\n",
    "In lectures we learned a construction of frequentist intervals (the Neyman construction) that is designed to acheive a good coverage. In this notebook, we're going to apply this method to our simple counting experiment and calculate a frequentist interval, and calculate  the coverage of the intervals. We'll also compare to the interval one would get from applying Wilks' theorem. \n",
    "\n",
    "## Neyman intervals with profiled likelihood\n",
    "\n",
    "We'll start with the Neyman construction for our interval. In this case, we'll calculate a 68% interval (commonly refered to as a 1$\\sigma$ interval).\n",
    "\n",
    "First, let's define our counting experiment model by importing the python file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f5f4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy\n",
    "from scipy.optimize import minimize\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "from counting_model import * "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e100170f",
   "metadata": {},
   "source": [
    "We need to define our test-statistic. We will use the one based on the profiled likelihood ratio, i.e \n",
    "\n",
    "$$\n",
    "\\zeta_{\\mu} = q(\\mu,\\eta_{\\mu}) - q(\\hat{\\mu},\\hat{\\eta})\n",
    "$$\n",
    "if $\\hat{\\mu}\\geq 0$ or, \n",
    "$$\n",
    "\\zeta_{\\mu} = q(\\mu,\\eta_{\\mu}) - q(0,\\eta_{0})\n",
    "$$\n",
    "otherwise.\n",
    "\n",
    "We can define each of these terms as functions to help us keep track. Remember, we already used $q$ before, so we can take that from our `counting_model_functions.py` module. The following two functions are just designed to make it easier to use the `optimize` package from `scipy` but they both just call our original function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4b25f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from counting_model_functions import *\n",
    "\n",
    "def q_unconstrained(x, args):\n",
    "  mu, eta = x[0], x[1]\n",
    "  np, eta_p = args[0], args[1]\n",
    "  return q(mu,eta,np,eta_p)\n",
    "\n",
    "def q_constrained(x, args):\n",
    "  eta=x[0]\n",
    "  mu, np, eta_p = args[0], args[1], args[2]\n",
    "  return q(mu,eta,np,eta_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5dc301",
   "metadata": {},
   "source": [
    "And some functions to find the profiled values of $\\mu$ and $\\eta$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b1085b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the minimisation routines\n",
    "def profiled_eta(mu, np, eta_p):\n",
    "  init_params = [-3.0]\n",
    "  bounds = [(-5,5)]\n",
    "  res = minimize(q_constrained,init_params,args=[mu,np,eta_p],bounds=bounds)\n",
    "  return res.x[0]\n",
    "\n",
    "def global_min(np,eta_p):\n",
    "  init_params = [0.1,-3.]\n",
    "  bounds = [(-1,50),(-5,5)]\n",
    "  mle = minimize(q_unconstrained,init_params,args=[np,eta_p],bounds=bounds)\n",
    "  return mle.fun,mle.x[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2591656",
   "metadata": {},
   "source": [
    "Finally, we have all of the pieces to define our test-statistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d23fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate test statistic\n",
    "@numpy.vectorize\n",
    "def zetamu(np,eta_p,mu):\n",
    "  q_value        = q(mu,profiled_eta(mu,np,eta_p),np,eta_p)\n",
    "  q_min,mu_min   = global_min(np,eta_p)\n",
    "  if mu_min < 0     : return q_value-q(0,profiled_eta(0,np,eta_p),np,eta_p)\n",
    "  else              : return q_value-q_min"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f313f43f",
   "metadata": {},
   "source": [
    "Remember, we want to know, for each value of our parameter of interest ($\\mu$), the part of the test-statistic distribution that contains 68% of possible outcomes of the test-statistic. \n",
    "\n",
    "For our test-statistic, this is the same as finding a single value of the test-statistic (call it $\\zeta_{68}$) for which 68% of  $\\zeta$ values are below. Remember, this is specified for each value of $\\mu$. We'll define a function to generate toys so that we can determine the distribution of $\\zeta$ and  $\\zeta_{68}$. \n",
    "\n",
    "Notice that our toys consist of pairs of observations. One  is a random value of the observed  number of events $n$, and the other is a *observed* value of $\\eta^{\\prime}$. Usually this value is taken to be 0 but since it is too a random variable, it has a distribution. In HEP, we typically think of this as representing other measurements that we could have made for the parameter $\\eta$. \n",
    "\n",
    "We need to choose a pdf to generate these $\\eta^{\\prime}$ from. Of course it should be a normal distribution as that is the term in the likelihood. We need a central value for this normal distribution and the usual convention is to use the profiled value of $\\eta=\\eta_{mu}$ - i.e it will be a different value for each $\\mu$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf62de3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return a set of heights for each true value of mu\n",
    "# for each value, we would mark on where 68% of the distribution lives\n",
    "\n",
    "def histo_zetamu(mu):\n",
    "  # find the best (profiled) nuisance parameter values for the data (n,0)\n",
    "  eta_profiled = profiled_eta(mu,n,0)\n",
    "  ntoys = 1000\n",
    "  toy_n   = numpy.random.poisson(lamb(mu,eta_profiled),size=ntoys)\n",
    "  toy_eta = numpy.random.normal(eta_profiled,1,size=ntoys)\n",
    "  zetamu_dist = zetamu(toy_n,toy_eta,mu)\n",
    "  #zetamu_dist = [zetamu(np,eta_p,mu) for np,eta_p in zip(toy_n,toy_eta)]\n",
    "  zetamu_dist.sort() ; zeta_68 = zetamu_dist[int(ntoys*0.68)]\n",
    "  return zetamu_dist, zeta_68"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f2773f",
   "metadata": {},
   "source": [
    "Let's look at the distribution for a single  value of $\\mu=2$. We'll print out the value of $\\zeta_{68}$ and the  value of  $\\zeta$ using the observed values of $n=2$ and $\\eta^{\\prime}=0$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d2aaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "zeta_obs_2 = zetamu(n,0,2)\n",
    "\n",
    "distribution_2, zeta_68_2 =  histo_zetamu(2)\n",
    "plt.hist(distribution_2,bins=20)\n",
    "plt.xlabel(\"$\\zeta_{2}$\")\n",
    "plt.show()\n",
    "\n",
    "print(\"zeta_68=\",zeta_68_2)\n",
    "print(\"zeta_obs=\",zeta_obs_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8088567",
   "metadata": {},
   "source": [
    "In this case, the observed value `zeta_obs` is less  than `zeta_68`. This means that the observed value is contained in the part of the distribution that contains 68% of the outcome! So we would accept this point ($\\mu=2$) into our 68% interval. \n",
    "\n",
    "Now let's repeat for a range of $\\mu$ values and report the full interval (i.e the range of points we accept). Note that this takes a little while to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c860a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_range   = numpy.arange(0.1,10,0.2)\n",
    "zeta_range = numpy.arange(0.1,10,0.2)\n",
    "zeta_obs_vals = []\n",
    "zeta_68_vals  = []\n",
    "mu_interval = []\n",
    "densities = []\n",
    "\n",
    "for mu_test in mu_range:\n",
    "  zeta_obs = zetamu(n,0,mu_test)\n",
    "  zetamu_toys,zeta_68 = histo_zetamu(mu_test)\n",
    "\n",
    "  # we're also keeping a record of all the distributions\n",
    "  density_vals = plt.hist(zetamu_toys,density=True,bins=zeta_range)\n",
    "  densities.append(density_vals[0])\n",
    "\n",
    "  zeta_68_vals.append(zeta_68)\n",
    "  zeta_obs_vals.append(zeta_obs)\n",
    "  if zeta_obs < zeta_68 : mu_interval.append(mu_test)\n",
    "\n",
    "plt.clf()\n",
    "# Print out the results\n",
    "mu_l, mu_u = min(mu_interval),max(mu_interval)\n",
    "mu_hat = global_min(n,0)[1]\n",
    "print(\"interval -> (%.2f,%.2f)\"%(mu_l,mu_u),\", mu = %.2f + %.2f -%.2f \"%(mu_hat,mu_u-mu_hat,mu_hat-mu_l))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7629503",
   "metadata": {},
   "source": [
    "We can plot the distribution of $\\zeta_{\\mu}$ for each of our $\\mu$ values that we tested. The colour map below shows the density of our test-statistic disribution. We also can overlay the values of $\\zeta_{68}$ that we determined. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0596c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X,Y = numpy.meshgrid(zeta_range,mu_range)\n",
    "c = plt.pcolor(X,Y,densities, \\\n",
    "    norm=LogNorm(vmin=0.001, vmax=2.0))\n",
    "plt.colorbar(c)\n",
    "plt.plot(zeta_obs_vals,mu_range,color='black',linewidth=3)\n",
    "plt.plot(zeta_68_vals,mu_range,color='red',linewidth=3)\n",
    "plt.xlabel(\"$\\zeta_{\\mu}$\")\n",
    "plt.ylabel(\"$\\\\mu$\")\n",
    "plt.title(\"$f(\\zeta_{\\mu}|H(\\mu))$\")\n",
    "plt.show()\n",
    "plt.savefig(\"frequentist_interval.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da7b2e4",
   "metadata": {},
   "source": [
    "You can see some odd structures here at small values of $\\mu$, that seem to smooth out at larger values. Also the value of  $\\zeta_{68}$ seems to settle down at larger values. This will be important for our next topic on coverage and Wilks'  theorem. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a33c1a",
   "metadata": {},
   "source": [
    "## Coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131d359b-cab7-41a0-905c-56ee6bcc1d25",
   "metadata": {},
   "source": [
    "We can check what the coverage of our methods (say for the 68.3\\% interval) by determining the *fraction of intervals* in $\\mu$, as a function of the true value $\\mu_0$, that contain $\\mu_{0}$. \n",
    "\n",
    "It sounds like a rather painful ordeal given that calculating a single interval can take time, however, we do not need to calculate each interval to figure out the coverage. \n",
    "\n",
    "Remember that $\\mu$ is included in the interval provided $\\zeta^{\\mathrm{obs}}_{\\mu}\\leq\\zeta^{68.3}_{\\mu}$. For the Neyman construction, we can use toys to calculate $\\zeta^{68.3}_{\\mu}$, while for the MINOS method, we assume $\\zeta^{68.3}_{\\mu}=1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4b1176-7526-446a-9ef4-5c1e1b1b0188",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_t683_neyman(mu):\n",
    "  eta_profiled = (profiled_eta(mu,n,0))\n",
    "  ntoys = 1000\n",
    "  toy_n   = numpy.random.poisson(lamb(mu,eta_profiled),size=ntoys)\n",
    "  toy_eta = numpy.random.normal(eta_profiled,1,size=ntoys)\n",
    "  zetamu_dist = numpy.array([zetamu(np,eta_p,mu) for np,eta_p in zip(toy_n,toy_eta)])\n",
    "  zetamu_dist.sort() ; zeta_68 = zetamu_dist[int(ntoys*0.683)]\n",
    "  return zeta_68\n",
    "    \n",
    "mu_range   = numpy.arange(0,12,0.2)\n",
    "n_coverage_toys = 1000\n",
    "\n",
    "cov_Neyman, cov_MINOS = [], []\n",
    "for mu_test in mu_range:\n",
    "       #Â Generate from the profiled value of the nuisance parameter\n",
    "       # we could also test the coverage under other values!\n",
    "       eta_profiled = profiled_eta(mu_test,n,0)\n",
    "       toy_n   = numpy.random.poisson(lamb(mu_test,eta_profiled),size=n_coverage_toys)\n",
    "       toy_eta = numpy.random.normal(eta_profiled,1,size=n_coverage_toys)\n",
    "\n",
    "       t683_Neyman = get_t683_neyman(mu_test)\n",
    "       t683_MINOS  = 1\n",
    "       zeta_obs_vals = numpy.array([ zetamu(np,eta_p,mu_test) for np,eta_p in zip(toy_n,toy_eta) ])\n",
    "       coverage_Neyman = float(len([x for x in zeta_obs_vals if x <= t683_Neyman ]))/n_coverage_toys\n",
    "       coverage_MINOS  = float(len([x for x in zeta_obs_vals if x <= t683_MINOS  ]))/n_coverage_toys\n",
    "       cov_Neyman.append(coverage_Neyman)\n",
    "       cov_MINOS.append(coverage_MINOS)\n",
    "\n",
    "plt.plot(mu_range,cov_Neyman,color='red',marker=\"o\",label=\"Neyman\")\n",
    "plt.plot(mu_range,cov_MINOS,color='blue',marker=\"o\",label=\"MINOS\")\n",
    "plt.plot(mu_range,[0.683 for m in mu_range],color='black',linestyle=\"--\")\n",
    "plt.xlabel(\"True value - $\\\\mu$\")\n",
    "plt.ylabel(\"Coverage\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.savefig(\"coverage_example.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65df9371-1a62-4587-af6e-56712447bda8",
   "metadata": {},
   "source": [
    "You can see that the Neyman construction gives very close coverage to the desired 68.3\\% except at very small $\\mu$, where the discrete nature of the Poisson distribution makes it difficult to find the exact $\\zeta^{68.3}_{\\mu}$ - in fact this is still seen at larger values, though the effect gets reduced. Instead, the MINOS method, jumps between over-coverage and undercoverage, eventually settling down only above $\\mu\\sim 8$.\n",
    "\n",
    "Below we plot the distribution of $\\zeta_{\\mu}$ at $\\mu=0.1$ and $\\mu=9$. For the larger value, away from the boundary the approximation of a $\\chi^{2}(1)$ is much more accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1dce8f-82a1-493d-b939-f5e99a6b5264",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2\n",
    "\n",
    "fig, (ax1,ax2) = plt.subplots(1,2,figsize=(14,6))\n",
    "\n",
    "chi2vals = chi2.pdf(numpy.arange(0.05,12.05,0.1),df=1)\n",
    "\n",
    "ax1.hist(histo_zetamu(0.1)[0], \\\n",
    "    bins=numpy.arange(0,12,0.1),range=(0,12),\\\n",
    "    color='black',histtype='step',density=True)\n",
    "ax1.plot(numpy.arange(0.05,12.05,0.1),chi2vals,color='red',label='$\\chi^{2}(1)$')\n",
    "ax2.hist(histo_zetamu(9)[0], \\\n",
    "    bins=numpy.arange(0,12,0.1),range=(0,12),\\\n",
    "    color='black',histtype='step',density=True)\n",
    "ax2.plot(numpy.arange(0.05,12.05,0.1),chi2vals,color='red',label='$\\chi^{2}(1)$')\n",
    "\n",
    "ax1.set_xlabel(\"$\\zeta_{0.1}$\")\n",
    "ax1.set_ylabel(\"$f(\\zeta_{0.1}|H(0.1))$\")\n",
    "ax2.set_xlabel(\"$\\zeta_{9}$\")\n",
    "ax2.set_ylabel(\"$f(\\zeta_{9}|H(9))$\")\n",
    "ax1.set_yscale('log')\n",
    "ax2.set_yscale('log')\n",
    "\n",
    "ax1.legend()\n",
    "plt.show()\n",
    "plt.savefig(\"tmu_dists_example.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d57b942-d796-49e9-bd1c-f74696d4be1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
