{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Galton Machine\n",
    "\n",
    "In the lectures, we discussed the central limit theorem. A Galton machine is as a simple demonstration of how this works using simulated events. The machine works as follows. \n",
    "\n",
    "There's a board with rows of pins with collection buckets (bins) at the bottom. A marble is dropped into the top of the board and left to fall to the bottom. When the marble encounters a pin it will either get deflected one spot to the left or one to the right. Finally when the marble leaves the board at the bottom, it will land in one of the buckets and the position of the bucket that it lands in is recorded. The figure below shows the setup. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![GaltonMachine](figs/galton_image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below simulates this process and counts the number of marbles that landed in each position. In this code, we record the \"normalised position\" which is just the position divided by the square root of the number of layers, and we bin that in a histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size': 10})\n",
    "\n",
    "# 50-50 probability to go left or right\n",
    "def galton_step(x):\n",
    " r = numpy.random.uniform(0,1)\n",
    " if r < 0.5: return x-1\n",
    " else: return x+1\n",
    "\n",
    "n_layers = 50\n",
    "n_trials = 10000\n",
    "\n",
    "finish=[]\n",
    "\n",
    "for i in range(n_trials):\n",
    "  start=0\n",
    "  for j in range(n_layers): start = galton_step(start)\n",
    "  # normalise the position \n",
    "  start /=  numpy.math.sqrt(n_layers)\n",
    "  finish.append(start)\n",
    "\n",
    "\n",
    "plt.hist(finish,100,(-3,3),density=True,color='green')\n",
    "plt.xlabel(\"position\")\n",
    "plt.ylabel(\"number of counters\")\n",
    "plt.title(\"N layers = %d, N trials = %d\"%(n_layers, n_trials))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you think about the distribution of \"position\", its simly a sum over $X_i$ where $X_i=-1$ or $X_i=+1$ with 50:50 probability. The distribution of each random variable $X_i$ is identical - a uniform distribution in the set $\\left\\{-1,1\\right\\}$. The normalised position $P_{N}$ is then given by, \n",
    "\n",
    "$$\n",
    "P_{N} = \\frac{1}{\\sqrt{N}} \\sum_{i} X_{i}.\n",
    "$$\n",
    "\n",
    "The mean of each distribution of $X_{i}$ is 0, and the variance is $(-1)^2\\times \\frac{1}{2} + (1)^2\\times \\frac{1}{2}=1$. The quantity we saw in lectures, \n",
    "\n",
    "$$\n",
    "T_{N} = \\frac{\\bar{X}N - \\sum_{i}\\mu_{1}^{i}}{\\sqrt{\\sum_{i}v^i_2}} = \\frac{\\frac{N}{N}\\sum_{i}X_{i} - 0 }{\\sqrt{N}} = P_{N}\n",
    "$$\n",
    "\n",
    "So of course as $N\\rightarrow+\\infty$ we know from the CLT that $P_{N}$ will be distributed as $\\phi(0,1)$\n",
    "\n",
    "Try increasing the number of layers `n_layers`. The larger it is, the more normal it will be."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
