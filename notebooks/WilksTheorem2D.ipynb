{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f357f285-85ca-4c56-aaab-6e37dbc5438f",
   "metadata": {},
   "source": [
    "# Wilks' Theorem\n",
    "\n",
    "In lectures, we saw how Wilks' theorem can be used to obtain intervals when using a profiled likelihood ratio as our test statistic. \n",
    "\n",
    "Suppose we have an analysis in which the number of events in bins of a distribution is counted. There is a background process and two signal processes with signal strength modifiers $\\mu_1$ and $\\mu_2$, with no restrictions on their range. Our log-likelihood function for this will be, \n",
    "\n",
    "$$\n",
    "    q(\\mu_1,\\mu_2,\\eta) = -2 \\left(\\sum_{i} n\\ln{\\lambda_{i}}- \\lambda_{i} \\right) + \\delta_{b}^{2},\n",
    "$$\n",
    "\n",
    "where $i$ runs over the bins, $\\lambda_{i}=\\mu_{1}s_{i,1}+\\mu_{2}s_{i,2}+b_{i}(1+k_{i})^{\\delta_{b}}$, and $\\delta_{b}$ is a nuisance parameter which can change the slope of the background (due to the $\\kappa_{i}$ being different for each bin).\n",
    "\n",
    "\n",
    "The background, signal and data distributions are shown below\n",
    "\n",
    "![LikelihoodShapeFit](figs/data_model_plot.png)\n",
    "\n",
    "The model is defined by specifying the bin contents, and the nominal value of our nuisance parameter as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e44b275-7962-4f36-919b-188f006dadd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size': 10})\n",
    "\n",
    "signal1_counts    = numpy.array([0,0,0,0.5,1,2,3,4,5,4.5,4.0,3.5,3.0,2.5,2,1.5,1,0.5,0,0,0,0,0,0,0])\n",
    "signal2_counts    = numpy.array([0,0,1,2,3,4,5,6,5,3,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0])\n",
    "background_counts = numpy.array([60*numpy.exp(-0.1*i) for i in range(len(signal1_counts))])\n",
    "bkg_uncert        = numpy.array([0.3,0.3,0.3,0.2,0.2,0.2,0.2,0.2,0.2,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.05,0.05,0.05,0.05,0.02,0.02,0.02,0.02])\n",
    "\n",
    "nbins = len(signal1_counts)\n",
    "\n",
    "# data = [[bin_contents],delta_b_0] \n",
    "data = [numpy.array([65, 45, 47, 37, 37, 40, 42, 36, 34, 36, 22, 23, 23, 18, 17, 13, 12, 12, 11, 12, 6, 6, 6, 9, 4]), 0]\n",
    "\n",
    "nbins = len(data[0])\n",
    "\n",
    "# now define the Poisson means\n",
    "@numpy.vectorize\n",
    "def lamb(s1,s2,delta_b,b,k):\n",
    "  return s1+s2+b*(1+k)**delta_b\n",
    "\n",
    "@numpy.vectorize\n",
    "def log_poisson(data_in,signal1,signal2,background,bkg_u,mu1,mu2,delta_b):\n",
    "  l = lamb(mu1*signal1,mu2*signal2,delta_b,background,bkg_u)\n",
    "  return data_in*numpy.log(l) - l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f7a71d-2618-4d1f-baef-c48af5d24201",
   "metadata": {},
   "source": [
    "First we need to define our functions to evaluate $q$, and (since we are profiling) find the profiled value of our nuisance parameter $\\delta_{b}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d761b6ee-de5e-4a1c-9f33-518863c777ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "def q(data,mu1,mu2,delta_b):\n",
    "  constr = (data[1]-delta_b)**2\n",
    "  return constr -2*sum(log_poisson(data[0],signal1_counts,signal2_counts,\\\n",
    "                        background_counts,bkg_uncert,mu1,mu2,delta_b)) \n",
    "\n",
    "def q_unconstrained(x, args):\n",
    "  mu1, mu2, delta_b = x[0], x[1], x[2]\n",
    "  #data = args[0]\n",
    "  return q(args,mu1,mu2,delta_b)\n",
    "\n",
    "def q_constrained(x, args):\n",
    "  delta_b = x[0]\n",
    "  data, mu1, mu2 = args[0], args[1], args[2]\n",
    "  return q(data,mu1,mu2,delta_b)\n",
    "\n",
    "def profiled_nuis(data, mu1, mu2):\n",
    "  init_params = [-3.0]\n",
    "  bounds =  [(-5,5)]\n",
    "  res = minimize(q_constrained,init_params,args=[data,mu1,mu2],bounds=bounds)\n",
    "  return res.x[0]\n",
    "\n",
    "def global_min(data):\n",
    "  init_params = [0.1,0.1,-3.]\n",
    "  bounds = [(-10,10),(-10,10),(-5,5)]\n",
    "  mle = minimize(q_unconstrained,init_params,args=data,bounds=bounds)\n",
    "  return mle.fun,mle.x[0],mle.x[1],mle.x[2]\n",
    "\n",
    "def delta_q(data,mu1,mu2,q_min):\n",
    "  q_value        = q(data,mu1,mu2,profiled_nuis(data,mu1,mu2))\n",
    "  return q_value-q_min"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee3801e-9e8e-4ad5-a908-f7d88ba7456c",
   "metadata": {},
   "source": [
    "First, let's find the profiled value of our two  signal strengths (these are the values used in the figure above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14809658-36b1-43b8-b7e6-3e7f81a57b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_min,mu1_min,mu2_min,deltab_min   = global_min(data)\n",
    "print(\"minimum of q = \",q_min,\" for mu_1=\",mu1_min,\"mu_2=\",mu2_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4912b6d7-148e-499a-917b-0fd897ec594f",
   "metadata": {},
   "source": [
    "Next, we scan the value of, \n",
    "\n",
    "$$\n",
    "\\Delta q(\\mu_1,\\mu_2) =  q(\\mu_1,\\mu_2,\\hat{\\eta}_{\\mu_1,\\mu_2}) - q(\\hat{\\mu}_1,\\hat{\\mu}_2,\\hat{\\eta}).\n",
    "$$\n",
    "\n",
    "in  a 2D grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd889dd-92ca-44a0-9c87-c396d0ddc351",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu1_axis = numpy.linspace(-1,5,50)\n",
    "mu2_axis = numpy.linspace(-2,5,50)\n",
    "z = [ [delta_q(data,m1,m2,q_min) for m1 in mu1_axis] for m2 in mu2_axis]\n",
    "\n",
    "X,Y = numpy.meshgrid(mu1_axis,mu2_axis)\n",
    "c = plt.pcolor(X,Y,z)\n",
    "plt.colorbar(c)\n",
    "conts = plt.contour(X,Y,z,[2.3,5.99],colors=\"white\")\n",
    "plt.clabel(conts, fontsize=9, inline=1)\n",
    "plt.xlabel(\"$\\\\mu_{1}$\")\n",
    "plt.ylabel(\"$\\\\mu_{2}$\")\n",
    "plt.title(\"$\\Delta q(\\mu_{1},\\mu_{2})$\")\n",
    "plt.plot([mu1_min],[mu2_min],color=\"white\",marker=\"P\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffeb689-15e1-472c-bf4c-e83d4a2f1c3f",
   "metadata": {},
   "source": [
    "Using Wilks' theorem, the 68.3\\% and 95.4\\% confidence regions are the regions for which $ \\Delta q(\\mu_1,\\mu_2) < 2.3$ and  $ \\Delta q(\\mu_1,\\mu_2) < 5.99$ respectively, as indicated by the contours. If we are interested in only the first parameter, $\\mu_1$, then we can use the function,  \n",
    "\n",
    "$$\n",
    "    \\Delta q(\\mu_1) =  q(\\mu_1,\\hat{\\mu}_{2,mu_{1}},\\hat{\\eta}_{\\mu_1}) - q(\\hat{\\mu}_1,\\hat{\\mu}_2,\\hat{\\eta}).\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad13d68-b6f9-4435-85cd-dcdfb8f33606",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â this function constrains mu1 (scan mu1)\n",
    "def q_constrained_mu1(x, args):\n",
    "  delta_b = x[1]\n",
    "  mu2   = x[0]\n",
    "  data, mu1 = args[0], args[1]\n",
    "  return q(data,mu1,mu2,delta_b)\n",
    "\n",
    "def profiled(data, mu1):\n",
    "  init_params = [1.0,-3.0]\n",
    "  bounds =  [(-4,10),(-5,5)]\n",
    "  res = minimize(q_constrained_mu1,init_params,args=[data,mu1],bounds=bounds)\n",
    "  return res.x\n",
    "\n",
    "def delta_qmu1(data,mu1,q_min):\n",
    "  profiled_nuisances = profiled(data,mu1)\n",
    "  q_value        = q(data,mu1,profiled_nuisances[0],profiled_nuisances[1])\n",
    "  return q_value-q_min\n",
    "\n",
    "def delta_qmu1_fix(data,mu1,mu2,q_min):\n",
    "  profiled_nuisances = profiled_nuis(data,mu1,mu2)\n",
    "  q_value        = q(data,mu1,mu2,profiled_nuisances)\n",
    "  return q_value-q_min"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e85c0c-567d-4ec4-99aa-e522055cea9d",
   "metadata": {},
   "source": [
    "The 68.3\\% and 95.4\\% intervals are found as the region for which  $\\Delta q(\\mu_1)<1$ and $\\Delta q(\\mu_1)<4$, respectively. We can find these intersections using some code similar to that below, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172b8f06-6225-432a-bb54-dfab0865343e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_crossing(x1,y1,x2,y2,K):\n",
    "  m = (y2-y1)/(x2-x1)\n",
    "  c = y1-m*x1\n",
    "  return (K-c)/m,K\n",
    "\n",
    "def findIntervals(x,y,conts=[1,4]):\n",
    "  xx0,yy0 = x[0],y[0]\n",
    "  crossing_x = []\n",
    "  for xx,yy in zip(x[1:],y[1:]):\n",
    "    for K in conts:\n",
    "      if (yy < K and yy0 > K) or (yy > K and yy0 < K):\n",
    "        crossing_x.append(return_crossing(xx0,yy0,xx,yy,K))\n",
    "    xx0=xx\n",
    "    yy0=yy\n",
    "  return crossing_x\n",
    "\n",
    "# plotting\n",
    "mu1_axis = numpy.linspace(-1,5,50)\n",
    "z = [ delta_qmu1(data,m1,q_min) for m1 in mu1_axis]\n",
    "zf = [ delta_qmu1_fix(data,m1,mu2_min,q_min) for m1 in mu1_axis]\n",
    "plt.plot(mu1_axis,z)\n",
    "plt.plot(mu1_axis,zf,linestyle=\"--\")\n",
    "plt.xlabel(\"$\\\\mu_{1}$\")\n",
    "plt.ylabel(\"$\\Delta q(\\mu_{1})$\")\n",
    "intervals = findIntervals(mu1_axis,z,[1,4])\n",
    "[ plt.plot([x[0],x[0]],[0,x[1]],color='red') for x in intervals ]\n",
    "plt.ylim((0,14))\n",
    "[ plt.plot([mu1_axis[0],mu1_axis[-1]],[k,k],color='black',linestyle='--') for k in [1,4] ]\n",
    "plt.show()\n",
    "\n",
    "print(\"68.3%% interval = (%.2f,%.2f)\"%(intervals[1][0],intervals[2][0]))\n",
    "print(\"95.4%% interval = (%.2f,%.2f)\"%(intervals[0][0],intervals[3][0]))\n",
    "\n",
    "print(\"Usually written as  mu_1=%.2f, +%.2f -%.2f\"%(mu1_min,intervals[2][0]-mu1_min,mu1_min-intervals[1][0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0879c55a-dd4d-4d5b-9336-a1390a8f9697",
   "metadata": {},
   "source": [
    "And we obtain the interval by finding the crossings at 1 and 4. \n",
    "\n",
    "If we didn't profile $\\mu_{2}$, the intervals would be smaller, which shows the effect of correlations between parameters of interest. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a8414f-09ae-474c-9fce-b93a008b5bd9",
   "metadata": {},
   "source": [
    "## Hessian approximation\n",
    "\n",
    "Remember that in the lectures, we showed that expanding around the minimum of the likelihood yields a simple quadratic form. If we associate this with a Gaussian distribution for the parameters (or multi-normal distribution if there is more than one parameter of interest), then we can associate the variance with the second derivative of the negative log-likelihood.\n",
    "\n",
    "In 1D, if we assume an approximate Gaussian distribution for $\\mu$ we have that \n",
    "\n",
    "$$\n",
    "\\frac{1}{\\sigma^{2}} = \\frac{1}{2}q''(\\hat{\\mu}) = \\frac{1}{2}\\frac{d^{2}q}{d \\mu^{2}}|_{\\hat{\\mu}}\n",
    "$$\n",
    "\n",
    "In our case, since we have 2-dimensions, we generalise to \n",
    "\n",
    "$$\n",
    "(\\nu_{1,1}^{\\mu_{i},\\mu_{j}})^{-1} = \\frac{1}{2}\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial^2 q}{\\partial \\mu_{1}^{2}} & \\frac{\\partial^2 q}{\\partial \\mu_{1}\\partial \\mu_{2}} \\\\\n",
    "\\frac{\\partial^2 q}{\\partial \\mu_{2}\\partial \\mu_{1}} & \\frac{\\partial^2 q}{\\partial \\mu_{2}^{2}}\\\\\n",
    "\\end{bmatrix}_{(\\hat{\\mu}_{1},\\hat{\\mu}_{2})}\n",
    "$$\n",
    "\n",
    "And in our case, we can find (I leave it for you to show it!) \n",
    "\n",
    "$$\n",
    "\\frac{\\partial ^{2}q}{\\partial \\mu_{1}^{2}} = 2\\sum_{i}n_{i}\\frac{1}{\\lambda_{i}^{2}}\\left(\\frac{\\partial \\lambda_{i}}{\\partial \\mu_{1}}\\right)^{2}\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\frac{\\partial ^{2}q}{\\partial \\mu_{2}^{2}} = 2\\sum_{i}n_{i}\\frac{1}{\\lambda_{i}^{2}}\\left(\\frac{\\partial \\lambda_{i}}{\\partial \\mu_{2}}\\right)^{2}\n",
    "$$\n",
    "\n",
    "while  \n",
    "$$\n",
    "\\frac{\\partial^2 q}{\\partial \\mu_{1}\\partial \\mu_{2}} =  \\frac{\\partial^2 q}{\\partial \\mu_{2}\\partial \\mu_{1}} = 2\\sum_{i}n_{i}\\frac{1}{\\lambda_{i}^{2}}\\frac{\\partial \\lambda_{i}}{\\partial \\mu_{1}}\\frac{\\partial \\lambda_{i}}{\\partial \\mu_{2}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfd9557-bab5-4dec-89ca-1dc25d875f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we already found \n",
    "# deltab_min\n",
    "# mu1_min\n",
    "# mu2_min \n",
    "\n",
    "# note that d lambda_i/d mu1 = s_i,1 and d^2 lambda_i/d mu1^2 = 0 \n",
    "# and d^2 lambda_i/d mu2^2 = 0, and d^2 lambda_i/d mu1 d mu2 = 0\n",
    "\n",
    "def d2qdmu12():\n",
    " lambdas = lamb(mu1_min*signal1_counts,mu2_min*signal2_counts,\n",
    "                deltab_min,background_counts,bkg_uncert)\n",
    "\n",
    " res_sum = sum([n*((1./l)**2)*s1*s1\n",
    "                  for n,l,s1 in \n",
    "                zip(data[0],lambdas,signal1_counts)\n",
    "               ])\n",
    "    \n",
    " return 2*res_sum\n",
    "\n",
    "def d2qdmu22():\n",
    " lambdas = lamb(mu1_min*signal1_counts,mu2_min*signal2_counts,\n",
    "                deltab_min,background_counts,bkg_uncert)\n",
    "\n",
    " res_sum = sum([n*((1./l)**2)*s2*s2\n",
    "                  for n,l,s2 in \n",
    "                zip(data[0],lambdas,signal2_counts)\n",
    "               ])\n",
    "    \n",
    " return 2*res_sum\n",
    "\n",
    "def d2qdmu1dmu2():\n",
    " lambdas = lamb(mu1_min*signal1_counts,mu2_min*signal2_counts,\n",
    "                deltab_min,background_counts,bkg_uncert)\n",
    "\n",
    " res_sum = sum([n*((1./l)**2)*s1*s2\n",
    "                  for n,l,s1,s2 in \n",
    "                zip(data[0],lambdas,signal1_counts,signal2_counts)\n",
    "               ])\n",
    "    \n",
    " return 2*res_sum\n",
    "\n",
    "Hessian = numpy.matrix([ [0.5*d2qdmu12(),0.5*d2qdmu1dmu2()],\n",
    "                         [0.5*d2qdmu1dmu2(),0.5*d2qdmu22()]\n",
    "                       ])\n",
    "print(\"Hessian Matrix...\")\n",
    "print(Hessian)\n",
    "\n",
    "Inverse = Hessian.getI()\n",
    "print(\"Inverse Hessian...\")\n",
    "print(Hessian.getI())\n",
    "\n",
    "print(\"sigma_mu1 = \",(Inverse[0,0])**0.5)\n",
    "print(\"sigma_mu2 = \",(Inverse[1,1])**0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8bec3e-9f38-4daf-a397-50d1bbcc8786",
   "metadata": {},
   "source": [
    "You can see that comparing $\\sigma_{\\mu_{1}}$ to what we got from scanning the profiled log-likelihood gives a good approximation to the width of 68.3% CL interval we calculated.  I'll leave the comparison for $\\sigma_{\\mu_{1}}$ to you.\n",
    "\n",
    "\n",
    "Note that we've actually ignored the effect of the nuisance parameter $\\delta_{b}$ here and have assumed that the interval does not depend on its value (i.e the correlation of either $\\mu_{1}$ or $\\mu_{2}$ with $\\delta_{b}$ is small). You can modify the code above to include the nuisance parameter, by defining a function to determine $\\frac{\\partial^{2}q}{\\partial \\delta_{b}^{2}}$ and the mixed partial terms and include them in the Hessian. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
